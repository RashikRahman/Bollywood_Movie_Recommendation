{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Movie_ID, Title, Year, Genre]\n",
       "Index: []"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['Movie_ID','Title','Year','Genre']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {2000:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2000',\n",
    "       2001:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2001',\n",
    "       2002:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2002',    \n",
    "       2003:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2003',\n",
    "       2004:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2004',\n",
    "       2005:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2005',\n",
    "       2006:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2006',\n",
    "       2007:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2007'\n",
    "      }\n",
    "id = 1 \n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.findAll('table', {'class':'wikitable'})[1].tbody\n",
    "    rows = table.find_all('tr')\n",
    "    for i in range(1, len(rows)):\n",
    "        tds = rows[i].find_all('td')\n",
    "        values = [id,tds[0].text, key, tds[3].text]\n",
    "        id+=1\n",
    "        df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.to_csv('asd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {2008:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2008',\n",
    "       2009:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2009',\n",
    "       2010:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2010'\n",
    "      }\n",
    "id=1\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "            if len(tds)==5:\n",
    "                values = [id, tds[1].text, key, tds[4].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==4:\n",
    "                values = [id, tds[0].text, key, tds[3].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[2].text, key, tds[5].text.replace('\\n','')]\n",
    "\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {\n",
    "       2011:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2011'\n",
    "      }\n",
    "id=1\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "            if len(tds)==7:\n",
    "                values = [id, tds[2].text, key, tds[3].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==5:\n",
    "                values = [id, tds[0].text, key, tds[1].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[1].text, key, tds[2].text.replace('\\n','')]\n",
    "\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {\n",
    "       2012:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2012'\n",
    "      }\n",
    "id=1\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "\n",
    "            if len(tds)==5:\n",
    "                values = [id, tds[1].text, key, tds[2].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[2].text, key, tds[3].text.replace('\\n','')]\n",
    "            \n",
    "            elif len(tds)==4:\n",
    "                values = [id, tds[0].text, key, tds[1].text.replace('\\n','')]\n",
    "\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL = {\n",
    "       2017:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2017'\n",
    "      }\n",
    "\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})[1:]\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "            if len(tds)==8:\n",
    "                    values = [id, tds[2].text, key, tds[5].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==7:\n",
    "                values = [id, tds[1].text, key, tds[4].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[0].text, key, tds[3].text.replace('\\n','')]\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {\n",
    "       2013:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2013'\n",
    "      }\n",
    "id=1\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "\n",
    "            if len(tds)==5:\n",
    "                values = [id, tds[1].text, key, tds[2].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[2].text, key, tds[3].text.replace('\\n','')]\n",
    "            \n",
    "            elif len(tds)==4:\n",
    "                values = [id, tds[0].text, key, tds[1].text.replace('\\n','')]\n",
    "\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 5 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=tbl[4].tbody\n",
    "rows = table.find_all('tr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(rows)):\n",
    "    tds = rows[i].find_all('td')\n",
    "    if len(tds)==5:\n",
    "                values = [id, tds[1].text, key, tds[2].text.replace('\\n','')]\n",
    "\n",
    "    elif len(tds)==6:\n",
    "        values = [id, tds[2].text, key, tds[3].text.replace('\\n','')]\n",
    "\n",
    "    elif len(tds)==4:\n",
    "        values = [id, tds[0].text, key, tds[1].text.replace('\\n','')]\n",
    "    print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Akkad Bakkkad Bam Be Bo', 'Romance', 'Dweep Raj Kochhar[22]\\n']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[22].find_all('td')\n",
    "values = [tds[0].text, tds[1].text, tds[3].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "language": "python",
   "name": "python38032bit597afaf4331643018e9f41f479e364be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
