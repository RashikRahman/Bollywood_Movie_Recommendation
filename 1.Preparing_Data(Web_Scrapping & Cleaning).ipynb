{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Movie_ID, Title, Year, Genre]\n",
       "Index: []"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['Movie_ID','Title','Year','Genre']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {2000:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2000',\n",
    "       2001:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2001',\n",
    "       2002:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2002',    \n",
    "       2003:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2003',\n",
    "       2004:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2004',\n",
    "       2005:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2005',\n",
    "       2006:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2006',\n",
    "       2007:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2007'\n",
    "      }\n",
    "id = 1 \n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.findAll('table', {'class':'wikitable'})[1].tbody\n",
    "    rows = table.find_all('tr')\n",
    "    for i in range(1, len(rows)):\n",
    "        tds = rows[i].find_all('td')\n",
    "        values = [id,tds[0].text, key, tds[3].text]\n",
    "        id+=1\n",
    "        df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.to_csv('asd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {2008:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2008',\n",
    "       2009:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2009',\n",
    "       2010:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2010'\n",
    "      }\n",
    "id=1\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "            if len(tds)==5:\n",
    "                values = [id, tds[1].text, key, tds[4].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==4:\n",
    "                values = [id, tds[0].text, key, tds[3].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[2].text, key, tds[5].text.replace('\\n','')]\n",
    "\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = {\n",
    "       2011:'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2011'\n",
    "      }\n",
    "id=1\n",
    "for key, url in URL.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tbl = soup.findAll('table', {'class':'wikitable'})\n",
    "    for t in tbl:\n",
    "        table=t.tbody\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            tds = rows[i].find_all('td')\n",
    "            if len(tds)==7:\n",
    "                values = [id, tds[2].text, key, tds[3].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==5:\n",
    "                values = [id, tds[0].text, key, tds[1].text.replace('\\n','')]\n",
    "\n",
    "            elif len(tds)==6:\n",
    "                values = [id, tds[1].text, key, tds[2].text.replace('\\n','')]\n",
    "\n",
    "            id+=1\n",
    "            df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv('asd.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Bollywood_films_of_2017'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "tbl = soup.findAll('table', {'class':'wikitable'})[1:]\n",
    "for t in tbl:\n",
    "    table=t.tbody\n",
    "    rows = table.find_all('tr')\n",
    "    for i in range(1, len(rows)):\n",
    "        tds = rows[i].find_all('td')\n",
    "        if len(tds)==8:\n",
    "                values = [id, tds[2].text, 2017, tds[5].text.replace('\\n','')]\n",
    "\n",
    "        elif len(tds)==7:\n",
    "            values = [id, tds[1].text, 2017, tds[4].text.replace('\\n','')]\n",
    "\n",
    "        elif len(tds)==6:\n",
    "            values = [id, tds[0].text, 2017, tds[3].text.replace('\\n','')]\n",
    "        id+=1\n",
    "        df = df.append(pd.Series(values, index=columns), ignore_index=True)\n",
    "df.to_csv('asd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English Vinglish',\n",
       " '5\\n',\n",
       " 'Sridevi, Priya Anand, Mehdi Nebbou, Amitabh Bachchan[68]\\n']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[1].find_all('td')\n",
    "values = [tds[2].text, tds[1].text, tds[5].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunday', 'Action, Comedy\\n', 'Action, Comedy\\n']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[6].find_all('td')\n",
    "values = [tds[1].text, tds[4].text, tds[4].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Halla Bol', 'Social\\n', 'Social\\n']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[3].find_all('td')\n",
    "values = [tds[1].text, tds[4].text, tds[4].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U, Me Aur Ghar', 'Romance/Drama', '[29]\\n']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[8].find_all('td')\n",
    "values = [tds[0].text, tds[3].text, tds[5].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Runningshaadi.com', 'Comedy', '[30][31]\\n']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[9].find_all('td')\n",
    "values = [tds[1].text, tds[4].text, tds[6].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Ghazi Attack', 'Action', '[32][33]\\n']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[10].find_all('td')\n",
    "values = [tds[0].text, tds[3].text, tds[5].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Irada', 'Thriller/drama', '[34]\\n']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[11].find_all('td')\n",
    "values = [tds[0].text, tds[3].text, tds[5].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chauhar', 'Romance', '[35]\\n']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[12].find_all('td')\n",
    "values = [tds[0].text, tds[3].text, tds[5].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rangoon', 'War/drama', '[36][37]\\n']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = rows[13].find_all('td')\n",
    "values = [tds[1].text, tds[4].text, tds[6].text]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "language": "python",
   "name": "python38032bit597afaf4331643018e9f41f479e364be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
